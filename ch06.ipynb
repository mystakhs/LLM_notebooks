{
    "cells": [
     {
      "cell_type": "markdown",
      "id": "c024bfa4-1a7a-4751-b5a1-827225a3478b",
      "metadata": {
       "id": "c024bfa4-1a7a-4751-b5a1-827225a3478b"
      },
      "source": [
       "<table style=\"width:100%\">\n",
       "<tr>\n",
       "<td style=\"vertical-align:middle; text-align:left;\">\n",
       "<font size=\"2\">\n",
       "これは <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a>（<a href=\"https://sebastianraschka.com\">Sebastian Raschka</a>著）の補足コードである<br>\n",
       "<br>コードリポジトリ: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
       "</font>\n",
       "</td>\n",
       "<td style=\"vertical-align:middle; text-align:left;\">\n",
       "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "bfabadb8-5935-45ff-b39c-db7a29012129",
      "metadata": {
       "id": "bfabadb8-5935-45ff-b39c-db7a29012129"
      },
      "source": [
       "# 第6章: テキスト分類のファインチューニング"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5b7e01c2-1c84-4f2a-bb51-2e0b74abda90",
      "metadata": {
       "colab": {
        "base_uri": "https://localhost:8080/"
       },
       "id": "5b7e01c2-1c84-4f2a-bb51-2e0b74abda90",
       "outputId": "9495f150-9d79-4910-d6e7-6c0d9aae4a41"
      },
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "matplotlib version: 3.10.0\n",
         "numpy version: 2.0.2\n",
         "tiktoken version: 0.9.0\n",
         "torch version: 2.6.0\n",
         "tensorflow version: 2.18.0\n",
         "pandas version: 2.2.3\n"
        ]
       }
      ],
      "source": [
       "from importlib.metadata import version\n",
       "\n",
       "pkgs = [\"matplotlib\",  # Plotting library\n",
       "        \"numpy\",       # PyTorch & TensorFlow dependency\n",
       "        \"tiktoken\",    # Tokenizer\n",
       "        \"torch\",       # Deep learning library\n",
       "        \"tensorflow\",  # For OpenAI's pretrained weights\n",
       "        \"pandas\"       # Dataset loading\n",
       "       ]\n",
       "for p in pkgs:\n",
       "    print(f\"{p} version: {version(p)}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "a445828a-ff10-4efa-9f60-a2e2aed4c87d",
      "metadata": {},
      "source": [
       "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/chapter-overview.webp\" width=500px>"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "3a84cf35-b37f-4c15-8972-dfafc9fadc1c",
      "metadata": {
       "id": "3a84cf35-b37f-4c15-8972-dfafc9fadc1c"
      },
      "source": [
       "## 6.1 ファインチューニングの異なるカテゴリ"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "ede3d731-5123-4f02-accd-c670ce50a5a3",
      "metadata": {
       "id": "ede3d731-5123-4f02-accd-c670ce50a5a3"
      },
      "source": [
       "- このセクションにはコードが含まれていない"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "ac45579d-d485-47dc-829e-43be7f4db57b",
      "metadata": {},
      "source": [
       "- 言語モデルをファインチューニングする最も一般的な方法は、指示ファインチューニングと分類ファインチューニングである\n",
       "- 下図に示すインストラクションファインチューニングは次章のトピックである"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "6c29ef42-46d9-43d4-8bb4-94974e1665e4",
      "metadata": {},
      "source": [
       "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/instructions.webp\" width=500px>"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "a7f60321-95b8-46a9-97bf-1d07fda2c3dd",
      "metadata": {},
      "source": [
       "- この章のトピックである分類ファインチューニングは、もし機械学習の背景があればすでに馴染みがある手順である（たとえば、手書き数字を分類するために畳み込みネットワークを訓練するなど）。\n",
       "- 分類ファインチューニングでは、特定のクラスラベル（例:「spam」「not spam」）の数があり、モデルはそれらを出力できる\n",
       "- 分類ファインチューニングされたモデルは、学習時に見たクラス（たとえば「spam」「not spam」）のみを予測できるのに対し、インストラクションファインチューニングされたモデルは通常、多岐にわたるタスクを実行できる\n",
       "- 分類ファインチューニングされたモデルは非常に特化したモデルと考えられる。多くのタスクをこなす汎用モデルを作るより、特化したモデルを作るほうがはるかに容易である"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "0b37a0c4-0bb1-4061-b1fe-eaa4416d52c3",
      "metadata": {},
      "source": [
       "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/spam-non-spam.webp\" width=500px>"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "8c7017a2-32aa-4002-a2f3-12aac293ccdf",
      "metadata": {
       "id": "8c7017a2-32aa-4002-a2f3-12aac293ccdf"
      },
      "source": [
       "## 6.2 データセットの準備"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "5f628975-d2e8-4f7f-ab38-92bb868b7067",
      "metadata": {},
      "source": [
       "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/overview-1.webp\" width=500px>"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "9fbd459f-63fa-4d8c-8499-e23103156c7d",
      "metadata": {
       "id": "9fbd459f-63fa-4d8c-8499-e23103156c7d"
      },
      "source": [
       "- このセクションでは、分類ファインチューニングに使用するデータセットを準備する\n",
       "- 「spam」と「non-spam」のテキストメッセージを含むデータセットを使用して、モデルをそれらを分類可能にする\n",
       "- まず、データセットをダウンロードして解凍する"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 2,
      "id": "def7c09b-af9c-4216-90ce-5e67aed1065c",
      "metadata": {
       "colab": {
        "base_uri": "https://localhost:8080/"
       },
       "id": "def7c09b-af9c-4216-90ce-5e67aed1065c",
       "outputId": "424e4423-f623-443c-ab9e-656f9e867559"
      },
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "File downloaded and saved as sms_spam_collection/SMSSpamCollection.tsv\n"
        ]
       }
      ],
      "source": [
       "import urllib.request\n",
       "import zipfile\n",
       "import os\n",
       "from pathlib import Path\n",
       "\n",
       "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
       "zip_path = \"sms_spam_collection.zip\"\n",
       "extracted_path = \"sms_spam_collection\"\n",
       "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
       "\n",
       "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
       "    if data_file_path.exists():\n",
       "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
       "        return\n",
       "\n",
       "    # Downloading the file\n",
       "    with urllib.request.urlopen(url) as response:\n",
       "        with open(zip_path, \"wb\") as out_file:\n",
       "            out_file.write(response.read())\n",
       "\n",
       "    # Unzipping the file\n",
       "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
       "        zip_ref.extractall(extracted_path)\n",
       "\n",
       "    # Add .tsv file extension\n",
       "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
       "    os.rename(original_file_path, data_file_path)\n",
       "    print(f\"File downloaded and saved as {data_file_path}\")\n",
       "\n",
       "try:\n",
       "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
       "except (urllib.error.HTTPError, urllib.error.URLError, TimeoutError) as e:\n",
       "    print(f\"Primary URL failed: {e}. Trying backup URL...\")\n",
       "    url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/sms%2Bspam%2Bcollection.zip\"\n",
       "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path) "
      ]
     },
     {
      "cell_type": "markdown",
      "id": "6aac2d19-06d0-4005-916b-0bd4b1ee50d1",
      "metadata": {
       "id": "6aac2d19-06d0-4005-916b-0bd4b1ee50d1"
      },
      "source": [
       "- このデータセットはタブ区切りのテキストファイルとして保存されており、pandasのDataFrameに読み込むことができる"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 4,
      "id": "da0ed4da-ac31-4e4d-8bdd-2153be4656a4",
      "metadata": {
       "colab": {
        "base_uri": "https://localhost:8080/",
        "height": 423
       },
       "id": "da0ed4da-ac31-4e4d-8bdd-2153be4656a4",
       "outputId": "a16c5cde-d341-4887-a93f-baa9bec542ab"
      },
      "outputs": [
       {
        "data": {
         "text/html": [
          "<div>\n",
          "<style scoped>\n",
          "    .dataframe tbody tr th:only-of-type {\n",
          "        vertical-align: middle;\n",
          "    }\n",
          "\n",
          "    .dataframe tbody tr th {\n",
          "        vertical-align: top;\n",
          "    }\n",
          "\n",
          "    .dataframe thead th {\n",
          "        text-align: right;\n",
          "    }\n",
          "</style>\n",
          "<table border=\"1\" class=\"dataframe\">\n",
          "  <thead>\n",
          "    <tr style=\"text-align: right;\">\n",
          "      <th></th>\n",
          "      <th>Label</th>\n",
          "      <th>Text</th>\n",
          "    </tr>\n",
          "  </thead>\n",
          "  <tbody>\n",
          "    <tr>\n",
          "      <th>0</th>\n",
          "      <td>ham</td>\n",
          "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>1</th>\n",
          "      <td>ham</td>\n",
          "      <td>Ok lar... Joking wif u oni...</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>2</th>\n",
          "      <td>spam</td>\n",
          "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>3</th>\n",
          "      <td>ham</td>\n",
          "      <td>U dun say so early hor... U c already then say...</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>4</th>\n",
          "      <td>ham</td>\n",
          "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>...</th>\n",
          "      <td>...</td>\n",
          "      <td>...</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>5567</th>\n",
          "      <td>spam</td>\n",
          "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>5568</th>\n",
          "      <td>ham</td>\n",
          "      <td>Will ü b going to esplanade fr home?</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>5569</th>\n",
          "      <td>ham</td>\n",
          "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>5570</th>\n",
          "      <td>ham</td>\n",
          "      <td>The guy did some bitching but I acted like i'd...</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>5571</th>\n",
          "      <td>ham</td>\n",
          "      <td>Rofl. Its true to its name</td>\n",
          "    </tr>\n",
          "  </tbody>\n",
          "</table>\n",
          "<p>5572 rows × 2 columns</p>\n",
          "</div>"
         ],
         "text/plain": [
          "     Label                                               Text\n",
          "0      ham  Go until jurong point, crazy.. Available only ...\n",
          "1      ham                      Ok lar... Joking wif u oni...\n",
          "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
          "3      ham  U dun say so early hor... U c already then say...\n",
          "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
          "...    ...                                                ...\n",
          "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
          "5568   ham               Will ü b going to esplanade fr home?\n",
          "5569   ham  Pity, * was in mood for that. So...any other s...\n",
          "5570   ham  The guy did some bitching but I acted like i'd...\n",
          "5571   ham                         Rofl. Its true to its name\n",
          "\n",
          "[5572 rows x 2 columns]"
         ]
        },
        "execution_count": 4,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "import pandas as pd\n",
       "\n",
       "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
       "df"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "e7b6e631-4f0b-4aab-82b9-8898e6663109",
      "metadata": {
       "id": "e7b6e631-4f0b-4aab-82b9-8898e6663109"
      },
      "source": [
       "- クラス分布を確認すると、「ham」（非スパム）のほうが「spam」よりもはるかに多いことがわかる"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 5,
      "id": "495a5280-9d7c-41d4-9719-64ab99056d4c",
      "metadata": {
       "colab": {
        "base_uri": "https://localhost:8080/"
       },
       "id": "495a5280-9d7c-41d4-9719-64ab99056d4c",
       "outputId": "761e0482-43ba-4f46-f4b7-6774dae51b38"
      },
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Label\n",
         "ham     4825\n",
         "spam     747\n",
         "Name: count, dtype: int64\n"
        ]
       }
      ],
      "source": [
       "print(df[\"Label\"].value_counts())"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "f773f054-0bdc-4aad-bbf6-397621bf63db",
      "metadata": {
       "id": "f773f054-0bdc-4aad-bbf6-397621bf63db"
      },
      "source": [
       "- 学習の便宜上、そして教育用の目的として小さなデータセットを使うほうがよいので、データセットをサンプリングして各クラスを747件ずつに揃える\n",
       "- （不均衡なクラス分布を扱う方法は他にも多数存在するが、本書の焦点はLLMであるため割愛する。詳しくは [`imbalanced-learn` ユーザーガイド](https://imbalanced-learn.org/stable/user_guide.html)を参照）"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7be4a0a2-9704-4a96-b38f-240339818688",
      "metadata": {
       "colab": {
        "base_uri": "https://localhost:8080/"
       },
       "id": "7be4a0a2-9704-4a96-b38f-240339818688",
       "outputId": "396dc415-cb71-4a88-e85d-d88201c6d73f"
      },
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Label\n",
         "ham     747\n",
         "spam    747\n",
         "Name: count, dtype: int64\n"
        ]
       }
      ],
      "source": [
       "def create_balanced_dataset(df):\n",
       "    \n",
       "    # Count the instances of \"spam\"\n",
       "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
       "    \n",
       "    # Randomly sample \"ham\" instances to match the number of \"spam\" instances\n",
       "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
       "    \n",
       "    # Combine ham \"subset\" with \"spam\"\n",
       "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
       "\n",
       "    return balanced_df\n",
       "\n",
       "\n",
       "balanced_df = create_balanced_dataset(df)\n",
       "print(balanced_df[\"Label\"].value_counts())"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "d3fd2f5a-06d8-4d30-a2e3-230b86c559d6",
      "metadata": {
       "id": "d3fd2f5a-06d8-4d30-a2e3-230b86c559d6"
      },
      "source": [
       "- 次に、文字列であるクラスラベル「ham」「spam」を整数ラベル0と1に変換する:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c1b10c3d-5d57-42d0-8de8-cf80a06f5ffd",
      "metadata": {
       "id": "c1b10c3d-5d57-42d0-8de8-cf80a06f5ffd"
      },
      "outputs": [],
      "source": [
       "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e6f7f062-ef4e-4020-8275-71990cab4414",
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/html": [
          "<div>\n",
          "<style scoped>\n",
          "    .dataframe tbody tr th:only-of-type {\n",
          "        vertical-align: middle;\n",
          "    }\n",
          "\n",
          "    .dataframe tbody tr th {\n",
          "        vertical-align: top;\n",
          "    }\n",
          "\n",
          "    .dataframe thead th {\n",
          "        text-align: right;\n",
          "    }\n",
          "</style>\n",
          "<table border=\"1\" class=\"dataframe\">\n",
          "  <thead>\n",
          "    <tr style=\"text-align: right;\">\n",
          "      <th></th>\n",
          "      <th>Label</th>\n",
          "      <th>Text</th>\n",
          "    </tr>\n",
          "  </thead>\n",
          "  <tbody>\n",
          "    <tr>\n",
          "      <th>4307</th>\n",
          "      <td>0</td>\n",
          "      <td>Awww dat is sweet! We can think of something t...</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>4138</th>\n",
          "      <td>0</td>\n",
          "      <td>Just got to  &amp;lt;#&amp;gt;</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>4831</th>\n",
          "      <td>0</td>\n",
          "      <td>The word \"Checkmate\" in chess comes from the P...</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>4461</th>\n",
          "      <td>0</td>\n",
          "      <td>This is wishing you a great day. Moji told me ...</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>5440</th>\n",
          "      <td>0</td>\n",
          "      <td>Thank you. do you generally date the brothas?</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>...</th>\n",
          "      <td>...</td>\n",
          "      <td>...</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>5537</th>\n",
          "      <td>1</td>\n",
          "      <td>Want explicit SEX in 30 secs? Ring 02073162414...</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>5540</th>\n",
          "      <td>1</td>\n",
          "      <td>ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>5547</th>\n",
          "      <td>1</td>\n",
          "      <td>Had your contract mobile 11 Mnths? Latest Moto...</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>5566</th>\n",
          "      <td>1</td>\n",
          "      <td>REMINDER FROM O2: To get 2.50 pounds free call...</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>5567</th>\n",
          "      <td>1</td>\n",
          "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
          "    </tr>\n",
          "  </tbody>\n",
          "</table>\n",
          "<p>1494 rows × 2 columns</p>\n"
         ],
         "text/plain": [
          "      Label                                               Text\n",
          "4307      0  Awww dat is sweet! We can think of something t...\n",
          "4138      0                             Just got to  &lt;#&gt;\n",
          "4831      0  The word \"Checkmate\" in chess comes from the P...\n",
          "4461      0  This is wishing you a great day. Moji told me ...\n",
          "5440      0      Thank you. do you generally date the brothas?\n",
          "...     ...                                                ...\n",
          "5537      1  Want explicit SEX in 30 secs? Ring 02073162414...\n",
          "5540      1  ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...\n",
          "5547      1  Had your contract mobile 11 Mnths? Latest Moto...\n",
          "5566      1  REMINDER FROM O2: To get 2.50 pounds free call...\n",
          "5567      1  This is the 2nd time we have tried 2 contact u...\n",
          "\n",
          "[1494 rows x 2 columns]"
         ]
        },
        "execution_count": 8,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "balanced_df"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "5715e685-35b4-4b45-a86c-8a8694de9d6f",
      "metadata": {
       "id": "5715e685-35b4-4b45-a86c-8a8694de9d6f"
      },
      "source": [
       "- 次に、データフレームをランダムに分割し、訓練、バリデーション、テストの各サブセットを作成する関数を定義する"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 9,
      "id": "uQl0Psdmx15D",
      "metadata": {
       "id": "uQl0Psdmx15D"
      },
      "outputs": [],
      "source": [
       "def random_split(df, train_frac, validation_frac):\n",
       "    # Shuffle the entire DataFrame\n",
       "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
       "\n",
       "    # Calculate split indices\n",
       "    train_end = int(len(df) * train_frac)\n",
       "    validation_end = train_end + int(len(df) * validation_frac)\n",
       "\n",
       "    # Split the DataFrame\n",
       "    train_df = df[:train_end]\n",
       "    validation_df = df[train_end:validation_end]\n",
       "    test_df = df[validation_end:]\n",
       "\n",
       "    return train_df, validation_df, test_df\n",
       "\n",
       "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
       "# Test size is implied to be 0.2 as the remainder\n",
       "\n",
       "train_df.to_csv(\"train.csv\", index=None)\n",
       "validation_df.to_csv(\"validation.csv\", index=None)\n",
       "test_df.to_csv(\"test.csv\", index=None)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "a8d7a0c5-1d5f-458a-b685-3f49520b0094",
      "metadata": {},
      "source": [
       "## 6.3 データローダーの作成"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "7126108a-75e7-4862-b0fb-cbf59a18bb6c",
      "metadata": {
       "id": "7126108a-75e7-4862-b0fb-cbf59a18bb6c"
      },
      "source": [
       "- テキストメッセージは長さが異なる。バッチで複数のサンプルをまとめる場合、以下の2つの方法のいずれかを取れる:\n",
       "  1. データセットやバッチ内の最短メッセージにすべて切り詰める\n",
       "  2. データセットやバッチ内の最長メッセージに合わせてパディングする\n",
       "\n",
       "- ここでは2を選択し、データセット内の最長メッセージに合わせてパディングを行う\n",
       "- パディングには `<|endoftext|>` トークンを使う（第2章で述べたとおり）"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "0829f33f-1428-4f22-9886-7fee633b3666",
      "metadata": {},
      "source": [
       "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/pad-input-sequences.webp?123\" width=500px>"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 10,
      "id": "74c3c463-8763-4cc0-9320-41c7eaad8ab7",
      "metadata": {
       "colab": {
        "base_uri": "https://localhost:8080/"
       },
       "id": "74c3c463-8763-4cc0-9320-41c7eaad8ab7",
       "outputId": "b5b48439-32c8-4b37-cca2-c9dc8fa86563"
      },
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "[50256]\n"
        ]
       }
      ],
      "source": [
       "import tiktoken\n",
       "\n",
       "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
       "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "04f582ff-68bf-450e-bd87-5fb61afe431c",
      "metadata": {
       "id": "04f582ff-68bf-450e-bd87-5fb61afe431c"
      },
      "source": [
       "- 以下の `SpamDataset` クラスは、訓練データセット内の最長シーケンスを特定し、それに合わせてパディングトークンを追加してシーケンス長を揃える"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d7791b52-af18-4ac4-afa9-b921068e383e",
      "metadata": {
       "id": "d7791b52-af18-4ac4-afa9-b921068e383e"
      },
      "outputs": [],
      "source": [
       "import torch\n",
       "from torch.utils.data import Dataset\n",
       "\n",
       "class SpamDataset(Dataset):\n",
       "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
       "        self.data = pd.read_csv(csv_file)\n",
       "\n",
       "        # Pre-tokenize texts\n",
       "        self.encoded_texts = [\n",
       "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
       "        ]\n",
       "\n",
       "        if max_length is None:\n",
       "            self.max_length = self._longest_encoded_length()\n",
       "        else:\n",
       "            self.max_length = max_length\n",
       "            # Truncate sequences if they are longer than max_length\n",
       "            self.encoded_texts = [\n",
       "                encoded_text[:self.max_length]\n",
       "                for encoded_text in self.encoded_texts\n",
       "            ]\n",
       "\n",
       "        # Pad sequences to the longest sequence\n",
       "        self.encoded_texts = [\n",
       "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
       "            for encoded_text in self.encoded_texts\n",
       "        ]\n",
       "\n",
       "    def __getitem__(self, index):\n",
       "        encoded = self.encoded_texts[index]\n",
       "        label = self.data.iloc[index][\"Label\"]\n",
       "        return (\n",
       "            torch.tensor(encoded, dtype=torch.long),\n",
       "            torch.tensor(label, dtype=torch.long)\n",
       "        )\n",
       "\n",
       "    def __len__(self):\n",
       "        return len(self.data)\n",
       "\n",
       "    def _longest_encoded_length(self):\n",
       "        max_length = 0\n",
       "        for encoded_text in self.encoded_texts:\n",
       "            encoded_length = len(encoded_text)\n",
       "            if encoded_length > max_length:\n",
       "                max_length = encoded_length\n",
       "        return max_length\n",
       "        # Note: A more pythonic version to implement this method\n",
       "        # is the following, which is also used in the next chapter:\n",
       "        # return max(len(encoded_text) for encoded_text in self.encoded_texts)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 12,
      "id": "uzj85f8ou82h",
      "metadata": {
       "colab": {
        "base_uri": "https://localhost:8080/"
       },
       "id": "uzj85f8ou82h",
       "outputId": "d08f1cf0-c24d-445f-a3f8-793532c3716f"
      },
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "120\n"
        ]
       }
      ],
      "source": [
       "train_dataset = SpamDataset(\n",
       "    csv_file=\"train.csv\",\n",
       "    max_length=None,\n",
       "    tokenizer=tokenizer\n",
       ")\n",
       "\n",
       "print(train_dataset.max_length)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "15bdd932-97eb-4b88-9cf9-d766ea4c3a60",
      "metadata": {},
      "source": [
       "- バリデーションセットとテストセットも訓練データの最長シーケンス長に合わせてパディングを行う\n",
       "- 訓練セット内の最長シーケンスより長いバリデーション／テストのサンプルは、`encoded_text[:self.max_length]` によって切り詰められる\n",
       "- この挙動は必須ではなく、もし望むならバリデーションセットとテストセットでも `max_length=None` を指定してそれぞれの最長を使うことも可能である"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bb0c502d-a75e-4248-8ea0-196e2b00c61e",
      "metadata": {
       "id": "bb0c502d-a75e-4248-8ea0-196e2b00c61e"
      },
      "outputs": [],
      "source": [
       "val_dataset = SpamDataset(\n",
       "    csv_file=\"validation.csv\",\n",
       "    max_length=train_dataset.max_length,\n",
       "    tokenizer=tokenizer\n",
       ")\n",
       "test_dataset = SpamDataset(\n",
       "    csv_file=\"test.csv\",\n",
       "    max_length=train_dataset.max_length,\n",
       "    tokenizer=tokenizer\n",
       ")"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "20170d89-85a0-4844-9887-832f5d23432a",
      "metadata": {},
      "source": [
       "- 次に、このデータセットを使ってデータローダーを生成する。これは以前の章でのデータローダー作成方法と同様である"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "64bcc349-205f-48f8-9655-95ff21f5e72f",
      "metadata": {},
      "source": [
       "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/batch.webp\" width=500px>"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8681adc0-6f02-4e75-b01a-a6ab75d05542",
      "metadata": {
       "colab": {
        "base_uri": "https://localhost:8080/"
       },
       "id": "8681adc0-6f02-4e75-b01a-a6ab75d05542",
       "outputId": "3266c410-4fdb-4a8c-a142-7f707e2525ab"
      },
      "outputs": [],
      "source": [
       "from torch.utils.data import DataLoader\n",
       "\n",
       "num_workers = 0\n",
       "batch_size = 8\n",
       "\n",
       "torch.manual_seed(123)\n",
       "\n",
       "train_loader = DataLoader(\n",
       "    dataset=train_dataset,\n",
       "    batch_size=batch_size,\n",
       "    shuffle=True,\n",
       "    num_workers=num_workers,\n",
       "    drop_last=True,\n",
       ")\n",
       "\n",
       "val_loader = DataLoader(\n",
       "    dataset=val_dataset,\n",
       "    batch_size=batch_size,\n",
       "    num_workers=num_workers,\n",
       "    drop_last=False,\n",
       ")\n",
       "\n",
       "test_loader = DataLoader(\n",
       "    dataset=test_dataset,\n",
       "    batch_size=batch_size,\n",
       "    num_workers=num_workers,\n",
       "    drop_last=False,\n",
       ")"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "ab7335db-e0bb-4e27-80c5-eea11e593a57",
      "metadata": {},
      "source": [
       "- 検証のため、データローダーをイテレートして、バッチ内に8つの訓練サンプルが含まれ、それぞれが120トークンであることを確認する"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4dee6882-4c3a-4964-af15-fa31f86ad047",
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Train loader:\n",
         "Input batch dimensions: torch.Size([8, 120])\n",
         "Label batch dimensions torch.Size([8])\n"
        ]
       }
      ],
      "source": [
       "print(\"Train loader:\")\n",
       "for input_batch, target_batch in train_loader:\n",
       "    pass\n",
       "\n",
       "print(\"Input batch dimensions:\", input_batch.shape)\n",
       "print(\"Label batch dimensions\", target_batch.shape)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "5cdd7947-7039-49bf-8a5e-c0a2f4281ca1",
      "metadata": {},
      "source": [
       "- 最後に、各データセットに含まれるバッチ数を出力する"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 16,
      "id": "IZfw-TYD2zTj",
      "metadata": {
       "colab": {
        "base_uri": "https://localhost:8080/"
       },
       "id": "IZfw-TYD2zTj",
       "outputId": "6934bbf2-9797-4fbe-d26b-1a246e18c2fb"
      },
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "130 training batches\n",
         "19 validation batches\n",
         "38 test batches\n"
        ]
       }
      ],
      "source": [
       "print(f\"{len(train_loader)} training batches\")\n",
       "print(f\"{len(val_loader)} validation batches\")\n",
       "print(f\"{len(test_loader)} test batches\")"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "d1c4f61a-5f5d-4b3b-97cf-151b617d1d6c",
      "metadata": {
       "id": "d1c4f61a-5f5d-4b3b-97cf-151b617d1d6c"
      },
      "source": [
       "## 6.4 事前学習済みの重みを用いたモデルの初期化"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "97e1af8b-8bd1-4b44-8b8b-dc031496e208",
      "metadata": {},
      "source": [
       "- このセクションでは、前章で扱った事前学習済みモデルを初期化する\n",
       "\n",
       "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/overview-2.webp\" width=500px>"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2992d779-f9fb-4812-a117-553eb790a5a9",
      "metadata": {
       "id": "2992d779-f9fb-4812-a117-553eb790a5a9"
      },
      "outputs": [],
      "source": [
       "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
       "INPUT_PROMPT = \"Every effort moves\"\n",
       "\n",
       "BASE_CONFIG = {\n",
       "    \"vocab_size\": 50257,     # Vocabulary size\n",
       "    \"context_length\": 1024,  # Context length\n",
       "    \"drop_rate\": 0.0,        # Dropout rate\n",
       "    \"qkv_bias\": True         # Query-key-value bias\n",
       "}\n",
       "\n",
       "model_configs = {\n",
       "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
       "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
       "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
       "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
       "}\n",
       "\n",
       "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
       "\n",
       "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
       "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
       "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
       "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
       ")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 18,
      "id": "022a649a-44f5-466c-8a8e-326c063384f5",
      "metadata": {
       "colab": {
        "base_uri": "https://localhost:8080/"
       },
       "id": "022a649a-44f5-466c-8a8e-326c063384f5",
       "outputId": "7091e401-8442-4f47-a1d9-ecb42a1ef930"
      },
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
         "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
         "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
         "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
         "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
         "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
         "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
        ]
       }
      ],
      "source": [
       "from gpt_download import download_and_load_gpt2\n",
       "from previous_chapters import GPTModel, load_weights_into_gpt\n",
       "# If the `previous_chapters.py` file is not available locally,\n",
       "# you can import it from the `llms-from-scratch` PyPI package.\n",
       "# For details, see: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n",
       "# E.g.,\n",
       "# from llms_from_scratch.ch04 import GPTModel\n",
       "# from llms_from_scratch.ch05 import download_and_load_gpt2, load_weights_into_gpt\n",
       "\n",
       "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
       "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
       "\n",
       "model = GPTModel(BASE_CONFIG)\n",
       "load_weights_into_gpt(model, params)\n",
       "model.eval();"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "ab8e056c-abe0-415f-b34d-df686204259e",
      "metadata": {},
      "source": [
       "- モデルが正しく読み込まれたことを確認するため、テキスト生成が正しく行われるかをチェックする"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 19,
      "id": "d8ac25ff-74b1-4149-8dc5-4c429d464330",
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Every effort moves you forward.\n",
         "\n",
         "The first step is to understand the importance of your work\n"
        ]
       }
      ],
      "source": [
       "from previous_chapters import (\n",
       "    generate_text_simple,\n",
       "    text_to_token_ids,\n",
       "    token_ids_to_text\n",
       ")\n",
       "\n",
       "# Alternatively:\n",
       "# from llms_from_scratch.ch05 import (\n",
       "#    generate_text_simple,\n",
       "#    text_to_token_ids,\n",
       "#    token_ids_to_text\n",
       "# )\n",
       "\n",
       "text_1 = \"Every effort moves you\"\n",
       "\n",
       "token_ids = generate_text_simple(\n",
       "    model=model,\n",
       "    idx=text_to_token_ids(text_1, tokenizer),\n",
       "    max_new_tokens=15,\n",
       "    context_size=BASE_CONFIG[\"context_length\"]\n",
       ")\n",
       "\n",
       "print(token_ids_to_text(token_ids, tokenizer))"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "69162550-6a02-4ece-8db1-06c71d61946f",
      "metadata": {},
      "source": [
       "- モデルを分類器としてファインチューニングする前に、プロンプトだけで既にスパムを分類できるか試してみる"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 20,
      "id": "94224aa9-c95a-4f8a-a420-76d01e3a800c",
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
         "\n",
         "The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n"
        ]
       }
      ],
      "source": [
       "text_2 = (\n",
       "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
       "    \" 'You are a winner you have been specially\"\n",
       "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
       ")\n",
       "\n",
       "token_ids = generate_text_simple(\n",
       "    model=model,\n",
       "    idx=text_to_token_ids(text_2, tokenizer),\n",
       "    max_new_tokens=23,\n",
       "    context_size=BASE_CONFIG[\"context_length\"]\n",
       ")\n",
       "\n",
       "print(token_ids_to_text(token_ids, tokenizer))"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "1ce39ed0-2c77-410d-8392-dd15d4b22016",
      "metadata": {},
      "source": [
       "- 見てわかるように、このモデルは指示に従うのがあまり得意ではない\n",
       "- これは事前学習のみであり、インストラクションファインチューニングは施されていないためである（インストラクションファインチューニングは次章で扱う）"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "4c9ae440-32f9-412f-96cf-fd52cc3e2522",
      "metadata": {
       "id": "4c9ae440-32f9-412f-96cf-fd52cc3e2522"
      },
      "source": [
       "## 6.5 分類ヘッドの追加"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "d6e9d66f-76b2-40fc-9ec5-3f972a8db9c0",
      "metadata": {},
      "source": [
       "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/lm-head.webp\" width=500px>"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "217bac05-78df-4412-bd80-612f8061c01d",
      "metadata": {},
      "source": [
       "- このセクションでは、事前学習済みLLMを改変して分類ファインチューニングを行えるようにする\n",
       "- まずはモデルのアーキテクチャを見直す"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   